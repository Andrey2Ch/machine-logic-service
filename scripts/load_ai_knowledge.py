#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –≤ PostgreSQL —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ embeddings.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python load_ai_knowledge.py

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - OPENAI_API_KEY –≤ environment
    - DATABASE_URL –≤ environment
    - –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω pgvector extension –≤ PostgreSQL
"""

import os
import json
import hashlib
import asyncio
from pathlib import Path
from typing import Optional

import asyncpg
import httpx

# OpenAI API –¥–ª—è embeddings
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
EMBEDDING_MODEL = "text-embedding-3-small"
EMBEDDING_DIM = 1536

# Database
DATABASE_URL = os.getenv("DATABASE_URL")

# Paths
KNOWLEDGE_BASE_PATH = Path(__file__).parent.parent.parent / "isramat-dashboard" / "src" / "lib" / "ai-assistant" / "knowledge"


def sha256_hash(content: str) -> str:
    """–í—ã—á–∏—Å–ª—è–µ—Ç SHA-256 —Ö—ç—à —Å—Ç—Ä–æ–∫–∏."""
    return hashlib.sha256(content.encode()).hexdigest()


async def get_embedding(text: str) -> list[float]:
    """–ü–æ–ª—É—á–∞–µ—Ç embedding –æ—Ç OpenAI API."""
    if not OPENAI_API_KEY:
        raise ValueError("OPENAI_API_KEY not set")
    
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "https://api.openai.com/v1/embeddings",
            headers={
                "Authorization": f"Bearer {OPENAI_API_KEY}",
                "Content-Type": "application/json"
            },
            json={
                "input": text[:8000],  # –õ–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤
                "model": EMBEDDING_MODEL
            },
            timeout=30.0
        )
        response.raise_for_status()
        data = response.json()
        return data["data"][0]["embedding"]


async def load_schema_knowledge(conn: asyncpg.Connection):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∑–Ω–∞–Ω–∏—è –æ —Å—Ö–µ–º–µ –ë–î."""
    schema_path = KNOWLEDGE_BASE_PATH / "schema" / "tables.json"
    
    if not schema_path.exists():
        print(f"‚ö†Ô∏è  Schema file not found: {schema_path}")
        return
    
    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ö–µ–º—ã –ë–î...")
    
    with open(schema_path, "r", encoding="utf-8") as f:
        schema = json.load(f)
    
    tables = schema.get("tables", {})
    
    for table_name, table_info in tables.items():
        title = f"–¢–∞–±–ª–∏—Ü–∞: {table_name}"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
        content_parts = [
            f"# {title}",
            f"\n{table_info.get('description', '')}",
            "\n## –ö–æ–ª–æ–Ω–∫–∏:"
        ]
        
        for col_name, col_info in table_info.get("columns", {}).items():
            col_type = col_info.get("type", "unknown")
            col_desc = col_info.get("description", "")
            content_parts.append(f"- **{col_name}** ({col_type}): {col_desc}")
        
        if "relations" in table_info:
            content_parts.append("\n## –°–≤—è–∑–∏:")
            for rel_name, rel_target in table_info["relations"].items():
                content_parts.append(f"- {rel_name} ‚Üí {rel_target}")
        
        if "business_rules" in table_info:
            content_parts.append("\n## –ë–∏–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª–∞:")
            for rule in table_info["business_rules"]:
                content_parts.append(f"- {rule}")
        
        if "examples" in table_info:
            content_parts.append("\n## –ü—Ä–∏–º–µ—Ä—ã SQL:")
            for example in table_info["examples"]:
                content_parts.append(f"```sql\n{example}\n```")
        
        content = "\n".join(content_parts)
        content_hash = sha256_hash(content)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç
        existing = await conn.fetchrow(
            "SELECT id, content_hash FROM ai_knowledge_documents WHERE document_type = 'schema' AND title = $1",
            title
        )
        
        if existing and existing["content_hash"] == content_hash:
            print(f"  ‚úÖ {table_name} - –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π")
            continue
        
        # –ü–æ–ª—É—á–∞–µ–º embedding
        print(f"  üìù {table_name} - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è embedding...")
        embedding = await get_embedding(content)
        
        metadata = {
            "table_name": table_name,
            "columns": list(table_info.get("columns", {}).keys()),
            "has_relations": bool(table_info.get("relations")),
        }
        
        if existing:
            # –û–±–Ω–æ–≤–ª—è–µ–º
            await conn.execute("""
                UPDATE ai_knowledge_documents 
                SET content = $1, content_hash = $2, embedding = $3, metadata = $4, updated_at = NOW()
                WHERE id = $5
            """, content, content_hash, embedding, json.dumps(metadata), existing["id"])
            print(f"  üîÑ {table_name} - –æ–±–Ω–æ–≤–ª–µ–Ω–æ")
        else:
            # –°–æ–∑–¥–∞—ë–º
            await conn.execute("""
                INSERT INTO ai_knowledge_documents (document_type, title, content, content_hash, embedding, metadata)
                VALUES ('schema', $1, $2, $3, $4, $5)
            """, title, content, content_hash, embedding, json.dumps(metadata))
            print(f"  ‚ú® {table_name} - —Å–æ–∑–¥–∞–Ω–æ")


async def load_markdown_knowledge(conn: asyncpg.Connection, doc_type: str, file_path: Path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∑–Ω–∞–Ω–∏—è –∏–∑ markdown —Ñ–∞–π–ª–∞."""
    if not file_path.exists():
        print(f"‚ö†Ô∏è  File not found: {file_path}")
        return
    
    print(f"üìÑ –ó–∞–≥—Ä—É–∑–∫–∞ {file_path.name}...")
    
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–µ–∫—Ü–∏–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º ##
    sections = []
    current_section = {"title": file_path.stem, "content": ""}
    
    for line in content.split("\n"):
        if line.startswith("## "):
            if current_section["content"].strip():
                sections.append(current_section)
            current_section = {
                "title": line[3:].strip(),
                "content": line + "\n"
            }
        elif line.startswith("# "):
            current_section["main_title"] = line[2:].strip()
            current_section["content"] += line + "\n"
        else:
            current_section["content"] += line + "\n"
    
    if current_section["content"].strip():
        sections.append(current_section)
    
    for section in sections:
        title = section.get("main_title", "") + " - " + section["title"] if section.get("main_title") else section["title"]
        content = section["content"].strip()
        
        if len(content) < 50:  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–µ–∫—Ü–∏–∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            continue
        
        content_hash = sha256_hash(content)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç
        existing = await conn.fetchrow(
            "SELECT id, content_hash FROM ai_knowledge_documents WHERE document_type = $1 AND title = $2",
            doc_type, title
        )
        
        if existing and existing["content_hash"] == content_hash:
            print(f"  ‚úÖ {title[:50]}... - –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π")
            continue
        
        # –ü–æ–ª—É—á–∞–µ–º embedding
        print(f"  üìù {title[:50]}... - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è embedding...")
        embedding = await get_embedding(content)
        
        metadata = {
            "source_file": str(file_path.name),
            "char_count": len(content),
        }
        
        if existing:
            await conn.execute("""
                UPDATE ai_knowledge_documents 
                SET content = $1, content_hash = $2, embedding = $3, metadata = $4, updated_at = NOW()
                WHERE id = $5
            """, content, content_hash, embedding, json.dumps(metadata), existing["id"])
            print(f"  üîÑ {title[:50]}... - –æ–±–Ω–æ–≤–ª–µ–Ω–æ")
        else:
            await conn.execute("""
                INSERT INTO ai_knowledge_documents (document_type, source_path, title, content, content_hash, embedding, metadata)
                VALUES ($1, $2, $3, $4, $5, $6, $7)
            """, doc_type, str(file_path), title, content, content_hash, embedding, json.dumps(metadata))
            print(f"  ‚ú® {title[:50]}... - —Å–æ–∑–¥–∞–Ω–æ")


async def load_sql_examples(conn: asyncpg.Connection):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã SQL –∑–∞–ø—Ä–æ—Å–æ–≤."""
    print("üíæ –ó–∞–≥—Ä—É–∑–∫–∞ SQL –ø—Ä–∏–º–µ—Ä–æ–≤...")
    
    examples = [
        {
            "question": "–°–∫–æ–ª—å–∫–æ –º–∞—à–∏–Ω–æ-—á–∞—Å–æ–≤ –Ω–∞—Ä–∞–±–æ—Ç–∞–ª –∫–∞–∂–¥—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä –∑–∞ –¥–µ–∫–∞–±—Ä—å?",
            "sql": """SELECT 
    e.full_name,
    COUNT(DISTINCT b.id) as batches,
    SUM(b.recounted_quantity) as parts,
    ROUND(SUM(b.recounted_quantity * sj.cycle_time / 3600.0), 1) as machine_hours
FROM batches b
JOIN setup_jobs sj ON b.setup_job_id = sj.id
JOIN employees e ON b.operator_id = e.id
WHERE b.batch_time >= '2025-12-01' AND b.batch_time < '2026-01-01'
GROUP BY e.full_name
ORDER BY machine_hours DESC""",
            "tables_used": ["batches", "setup_jobs", "employees"],
            "difficulty": "medium",
            "tags": ["–º–∞—à–∏–Ω–æ-—á–∞—Å—ã", "–æ–ø–µ—Ä–∞—Ç–æ—Ä—ã", "–æ—Ç—á—ë—Ç", "–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å"]
        },
        {
            "question": "–ö–∞–∫–∏–µ –ª–æ—Ç—ã —Å–µ–π—á–∞—Å –≤ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ?",
            "sql": """SELECT 
    l.lot_number,
    p.drawing_number,
    p.name as part_name,
    l.total_planned_quantity,
    l.created_at
FROM lots l
JOIN parts p ON l.part_id = p.id
WHERE l.status = 'in_production'
ORDER BY l.created_at DESC""",
            "tables_used": ["lots", "parts"],
            "difficulty": "simple",
            "tags": ["–ª–æ—Ç—ã", "–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ", "—Å—Ç–∞—Ç—É—Å"]
        },
        {
            "question": "–ü–æ–∫–∞–∂–∏ —Ç–æ–ø-5 –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –¥–µ—Ç–∞–ª–µ–π",
            "sql": """SELECT 
    e.full_name,
    SUM(b.recounted_quantity) as total_parts,
    COUNT(b.id) as batch_count
FROM batches b
JOIN employees e ON b.operator_id = e.id
WHERE b.batch_time >= NOW() - INTERVAL '30 days'
GROUP BY e.id, e.full_name
ORDER BY total_parts DESC
LIMIT 5""",
            "tables_used": ["batches", "employees"],
            "difficulty": "simple",
            "tags": ["—Ç–æ–ø", "–æ–ø–µ—Ä–∞—Ç–æ—Ä—ã", "–¥–µ—Ç–∞–ª–∏", "—Ä–µ–π—Ç–∏–Ω–≥"]
        },
        {
            "question": "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç–∞–Ω–∫—É SR-32 –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü",
            "sql": """SELECT 
    m.name as machine,
    COUNT(DISTINCT sj.id) as setups,
    COUNT(DISTINCT sj.lot_id) as lots,
    SUM(b.recounted_quantity) as total_parts,
    ROUND(SUM(b.recounted_quantity * sj.cycle_time / 3600.0), 1) as machine_hours,
    ROUND(AVG(sj.cycle_time), 1) as avg_cycle_time
FROM machines m
LEFT JOIN setup_jobs sj ON sj.machine_id = m.id AND sj.start_time >= NOW() - INTERVAL '30 days'
LEFT JOIN batches b ON b.setup_job_id = sj.id
WHERE m.name = 'SR-32'
GROUP BY m.id, m.name""",
            "tables_used": ["machines", "setup_jobs", "batches"],
            "difficulty": "medium",
            "tags": ["—Å—Ç–∞–Ω–æ–∫", "—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", "SR-32"]
        },
        {
            "question": "–ü–æ–∫–∞–∂–∏ –±—Ä–∞–∫ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–µ–¥–µ–ª—é",
            "sql": """SELECT 
    d.created_at::date as date,
    m.name as machine,
    l.lot_number,
    p.drawing_number,
    d.defect_quantity,
    d.reason
FROM defects d
JOIN setup_jobs sj ON d.setup_job_id = sj.id
JOIN machines m ON sj.machine_id = m.id
JOIN lots l ON sj.lot_id = l.id
JOIN parts p ON l.part_id = p.id
WHERE d.created_at >= NOW() - INTERVAL '7 days'
ORDER BY d.created_at DESC""",
            "tables_used": ["defects", "setup_jobs", "machines", "lots", "parts"],
            "difficulty": "medium",
            "tags": ["–±—Ä–∞–∫", "–¥–µ—Ñ–µ–∫—Ç—ã", "–∫–∞—á–µ—Å—Ç–≤–æ"]
        },
        {
            "question": "–†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –±–æ–ª—å—à–µ 10% –º–µ–∂–¥—É –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º –∏ –∫–ª–∞–¥–æ–≤—â–∏–∫–æ–º",
            "sql": """SELECT 
    b.batch_time::date as date,
    op.full_name as operator,
    wh.full_name as warehouse_employee,
    b.operator_reported_quantity,
    b.recounted_quantity,
    b.discrepancy_absolute,
    ROUND(b.discrepancy_percentage, 1) as discrepancy_pct
FROM batches b
JOIN employees op ON b.operator_id = op.id
LEFT JOIN employees wh ON b.warehouse_employee_id = wh.id
WHERE ABS(b.discrepancy_percentage) > 10
  AND b.batch_time >= NOW() - INTERVAL '30 days'
ORDER BY ABS(b.discrepancy_percentage) DESC""",
            "tables_used": ["batches", "employees"],
            "difficulty": "medium",
            "tags": ["—Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è", "–∫–æ–Ω—Ç—Ä–æ–ª—å", "—Å–∫–ª–∞–¥"]
        },
        {
            "question": "–°–∫–æ–ª—å–∫–æ —Å—Ç–∞–Ω–∫–æ–≤ —Å–µ–π—á–∞—Å —Ä–∞–±–æ—Ç–∞–µ—Ç?",
            "sql": """SELECT 
    COUNT(*) FILTER (WHERE sj.status = 'production') as working,
    COUNT(*) FILTER (WHERE sj.status = 'setup') as in_setup,
    COUNT(*) FILTER (WHERE sj.status = 'waiting_approval') as waiting_qc,
    (SELECT COUNT(*) FROM machines WHERE is_active = TRUE) as total_active
FROM setup_jobs sj
WHERE sj.end_time IS NULL""",
            "tables_used": ["setup_jobs", "machines"],
            "difficulty": "simple",
            "tags": ["—Å—Ç–∞–Ω–∫–∏", "—Å—Ç–∞—Ç—É—Å", "—Ä–∞–±–æ—Ç–∞"]
        },
        {
            "question": "–í—ã—Ä–∞–±–æ—Ç–∫–∞ –ø–æ —Å–º–µ–Ω–∞–º –∑–∞ —Å–µ–≥–æ–¥–Ω—è",
            "sql": """SELECT 
    CASE 
        WHEN EXTRACT(HOUR FROM b.batch_time) BETWEEN 6 AND 17 THEN '–°–º–µ–Ω–∞ 1 (–¥–µ–Ω—å)'
        ELSE '–°–º–µ–Ω–∞ 2 (–Ω–æ—á—å)'
    END as shift,
    COUNT(DISTINCT b.operator_id) as operators,
    SUM(b.recounted_quantity) as parts,
    ROUND(SUM(b.recounted_quantity * sj.cycle_time / 3600.0), 1) as machine_hours
FROM batches b
JOIN setup_jobs sj ON b.setup_job_id = sj.id
WHERE b.batch_time >= CURRENT_DATE
GROUP BY 1
ORDER BY 1""",
            "tables_used": ["batches", "setup_jobs"],
            "difficulty": "medium",
            "tags": ["—Å–º–µ–Ω—ã", "–≤—ã—Ä–∞–±–æ—Ç–∫–∞", "—Å–µ–≥–æ–¥–Ω—è"]
        },
    ]
    
    for ex in examples:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–æ–π –ø—Ä–∏–º–µ—Ä
        existing = await conn.fetchrow(
            "SELECT id FROM ai_sql_examples WHERE question = $1",
            ex["question"]
        )
        
        if existing:
            print(f"  ‚úÖ {ex['question'][:50]}... - —É–∂–µ –µ—Å—Ç—å")
            continue
        
        # –ü–æ–ª—É—á–∞–µ–º embedding –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞
        print(f"  üìù {ex['question'][:50]}... - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è embedding...")
        embedding = await get_embedding(ex["question"])
        
        await conn.execute("""
            INSERT INTO ai_sql_examples (question, question_embedding, sql_query, tables_used, difficulty, tags, is_verified)
            VALUES ($1, $2, $3, $4, $5, $6, TRUE)
        """, ex["question"], embedding, ex["sql"], ex["tables_used"], ex["difficulty"], ex["tags"])
        
        print(f"  ‚ú® {ex['question'][:50]}... - —Å–æ–∑–¥–∞–Ω–æ")


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –∑–Ω–∞–Ω–∏–π."""
    print("ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞\n")
    
    if not DATABASE_URL:
        print("‚ùå DATABASE_URL not set!")
        return
    
    if not OPENAI_API_KEY:
        print("‚ùå OPENAI_API_KEY not set!")
        return
    
    # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –ë–î
    conn = await asyncpg.connect(DATABASE_URL)
    
    try:
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ö–µ–º—É –ë–î
        await load_schema_knowledge(conn)
        print()
        
        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º glossary
        await load_markdown_knowledge(
            conn, "glossary", 
            KNOWLEDGE_BASE_PATH / "domain" / "glossary.md"
        )
        print()
        
        # 3. –ó–∞–≥—Ä—É–∂–∞–µ–º workflows
        await load_markdown_knowledge(
            conn, "workflow", 
            KNOWLEDGE_BASE_PATH / "domain" / "workflows.md"
        )
        print()
        
        # 4. –ó–∞–≥—Ä—É–∂–∞–µ–º SQL –ø—Ä–∏–º–µ—Ä—ã
        await load_sql_examples(conn)
        print()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = await conn.fetchrow("""
            SELECT 
                COUNT(*) as total,
                COUNT(*) FILTER (WHERE document_type = 'schema') as schema_count,
                COUNT(*) FILTER (WHERE document_type = 'glossary') as glossary_count,
                COUNT(*) FILTER (WHERE document_type = 'workflow') as workflow_count
            FROM ai_knowledge_documents
            WHERE is_active = TRUE
        """)
        
        sql_count = await conn.fetchval("SELECT COUNT(*) FROM ai_sql_examples")
        
        print("‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print(f"   üìä –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π: {stats['total']}")
        print(f"      - –°—Ö–µ–º–∞ –ë–î: {stats['schema_count']}")
        print(f"      - –ì–ª–æ—Å—Å–∞—Ä–∏–π: {stats['glossary_count']}")
        print(f"      - Workflows: {stats['workflow_count']}")
        print(f"   üíæ SQL –ø—Ä–∏–º–µ—Ä–æ–≤: {sql_count}")
        
    finally:
        await conn.close()


if __name__ == "__main__":
    asyncio.run(main())
